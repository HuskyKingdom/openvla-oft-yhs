# Energy Modelè®­ç»ƒé—®é¢˜åˆ†æä¸è§£å†³æ–¹æ¡ˆæŠ¥å‘Š

## é—®é¢˜æ€»ç»“

æ ¹æ®è®­ç»ƒæ›²çº¿å’Œä»£ç åˆ†æï¼ŒEnergy Modelå­˜åœ¨ä»¥ä¸‹å…³é”®é—®é¢˜ï¼š
1. **Energy Losså‘ˆç°ç›´çº¿**ï¼ˆå­¦ä¹ åœæ»ï¼‰
2. **è®­ç»ƒ18kæ­¥å·¦å³å‡ºç°NaN**
3. **æ— è®ºæ˜¯å¦warméƒ½å­˜åœ¨é—®é¢˜**
4. **Hinge Lossä¹Ÿæœ‰ç›¸åŒé—®é¢˜**

## æ ¹æœ¬åŸå› åˆ†æ

### 1. ğŸš¨ **èƒ½é‡å€¼èŒƒå›´è¿‡çª„é—®é¢˜** (æœ€ä¸¥é‡)

**é—®é¢˜æ ¹æº**:
```python
# åœ¨ EnergyModel.forward() ä¸­
E = self.act(raw) + 1e-6  # self.act = nn.Sigmoid()
```

**åˆ†æ**:
- èƒ½é‡å€¼è¢«å¼ºåˆ¶é™åˆ¶åœ¨ `(1e-6, 1.000001)` çš„æçª„èŒƒå›´
- å½“æ‰¹å†…æ ·æœ¬èƒ½é‡å€¼éƒ½èšé›†åœ¨ 0.4-0.6 ä¹‹é—´æ—¶ï¼ŒåŒºåˆ†åº¦ä¸¥é‡ä¸è¶³
- å¯¼è‡´InfoNCEæŸå¤±ä¸­çš„logitså‡ ä¹ç›¸åŒï¼Œæ¢¯åº¦æ¶ˆå¤±

**è¯æ®**:
```python
# In-batch InfoNCE è®¡ç®—
logits = (-E_ij) / tau  # tau = 0.5
# å½“ E_pos â‰ˆ 0.5, E_neg â‰ˆ 0.5 æ—¶
# logits â‰ˆ [-1.0, -1.0, -1.0, ...]  # æ‰€æœ‰å€¼å‡ ä¹ç›¸åŒï¼
```

### 2. ğŸ”¥ **Temperatureå‚æ•°ä¸å½“**

**é—®é¢˜**:
- `tau = 0.5` å¯¹äº (0,1) èŒƒå›´çš„èƒ½é‡å€¼è¿‡å°
- å¯¼è‡´logitså‹ç¼©è¿‡åº¦ï¼Œæ¢¯åº¦ä¿¡å·å¼±

**æ•°å€¼ç¤ºä¾‹**:
```
å‡è®¾ E_pos = 0.4, E_neg = 0.6
logits_pos = -0.4/0.5 = -0.8
logits_neg = -0.6/0.5 = -1.2
å·®å¼‚ä»…ä¸º 0.4ï¼Œå¯¹æ¯”å­¦ä¹ ä¿¡å·å¾ˆå¼±
```

### 3. âš¡ **æ¢¯åº¦æ¶ˆå¤±ä¸çˆ†ç‚¸**

**Sigmoidé¥±å’Œé—®é¢˜**:
```python
# å½“é¢„æµ‹å¤´è¾“å‡º raw å€¼è¿‡å¤§æˆ–è¿‡å°æ—¶
raw = [-10, 10]  # æç«¯æƒ…å†µ
E = sigmoid(raw) + 1e-6 = [1e-6, 1+1e-6]  
# sigmoidæ¢¯åº¦ â‰ˆ 0ï¼Œæ¢¯åº¦æ¶ˆå¤±
```

**æ¢¯åº¦çˆ†ç‚¸**:
- BÂ²å¤æ‚åº¦çš„energyè®¡ç®—å¯èƒ½å¯¼è‡´ç´¯ç§¯çš„æ•°å€¼è¯¯å·®
- ç¼ºä¹æœ‰æ•ˆçš„æ¢¯åº¦è£å‰ªèŒƒå›´æ§åˆ¶

### 4. ğŸ¯ **è´Ÿæ ·æœ¬è´¨é‡é—®é¢˜**

**In-batchæ–¹æ³•å±€é™**:
- ä¸¥é‡ä¾èµ–æ‰¹å†…æ ·æœ¬å¤šæ ·æ€§
- å¦‚æœåŒä¸€batchå†…åŠ¨ä½œç›¸ä¼¼åº¦é«˜ï¼Œè´Ÿæ ·æœ¬è´¨é‡å·®
- æ—©æœŸè®­ç»ƒé˜¶æ®µæ‰€æœ‰åŠ¨ä½œé¢„æµ‹éƒ½å¾ˆå·®ï¼Œè´Ÿæ ·æœ¬åŒºåˆ†åº¦ä½

### 5. ğŸ’¥ **æ•°å€¼ç²¾åº¦ç´¯ç§¯è¯¯å·®**

**ç±»å‹è½¬æ¢é—®é¢˜**:
```python
# åœ¨ energy_inbatch_swap_infonce ä¸­
h_rep = h.to(dtype).unsqueeze(1).expand(B, B, S, D).reshape(B*B, S, D)
# B=8æ—¶ï¼Œåˆ›å»º64ä¸ªå‰¯æœ¬ï¼Œå¯èƒ½å¼•å…¥æ•°å€¼è¯¯å·®
```

## è§£å†³æ–¹æ¡ˆ

### ğŸ› ï¸ **æ–¹æ¡ˆ1: ä¿®å¤èƒ½é‡å€¼èŒƒå›´** (ä¼˜å…ˆçº§ï¼šğŸ”´ æœ€é«˜)

**é—®é¢˜**: èƒ½é‡å€¼è¢«Sigmoidé™åˆ¶åœ¨(0,1)èŒƒå›´å¤ªçª„

**è§£å†³æ–¹æ¡ˆ**:
```python
class EnergyModel(nn.Module):
    def __init__(self, ...):
        # æ–¹æ¡ˆ1a: ç§»é™¤Sigmoidï¼Œä½¿ç”¨æ›´å¤§èŒƒå›´
        self.act = nn.Identity()  # æˆ–è€… nn.ReLU()
        self.energy_scale = 10.0  # èƒ½é‡ç¼©æ”¾å› å­
        
        # æ–¹æ¡ˆ1b: ä½¿ç”¨Softplusç¡®ä¿æ­£å€¼ä½†èŒƒå›´æ›´å¤§
        # self.act = nn.Softplus(beta=0.5)  # betaæ§åˆ¶é”åº¦
        
        # æ–¹æ¡ˆ1c: ä½¿ç”¨bounded activationä½†èŒƒå›´æ›´å¤§
        # self.energy_min, self.energy_max = 0.1, 10.0
    
    def forward(self, hN, a, pad_mask=None):
        # ... å‰é¢è®¡ç®—ç›¸åŒ ...
        raw = self.prediction_head(energy)
        
        # æ–¹æ¡ˆ1a: ç›´æ¥ç¼©æ”¾ + ReLUç¡®ä¿éè´Ÿ
        E = F.relu(raw) * self.energy_scale + 1e-3
        
        # æ–¹æ¡ˆ1b: Softplus
        # E = self.act(raw) + 1e-3
        
        # æ–¹æ¡ˆ1c: Bounded but wider range  
        # E = self.energy_min + (self.energy_max - self.energy_min) * torch.sigmoid(raw)
        
        return E
```

### ğŸŒ¡ï¸ **æ–¹æ¡ˆ2: è°ƒæ•´Temperatureå‚æ•°** (ä¼˜å…ˆçº§ï¼šğŸŸ  é«˜)

```python
def energy_inbatch_swap_infonce(
    energy_model, h, a_pos, pad_mask, 
    tau=2.0,  # å¢å¤§åˆ°2.0-5.0
    ...
):
    # æˆ–è€…ä½¿ç”¨è‡ªé€‚åº”æ¸©åº¦
    E_ij = energy_model(h_rep, a_rep, pm).view(B, B, 1).squeeze(-1)
    
    # è‡ªé€‚åº”æ¸©åº¦ï¼šåŸºäºèƒ½é‡å€¼çš„æ ‡å‡†å·®
    adaptive_tau = max(tau, E_ij.std().item() * 2.0)
    logits = (-E_ij) / adaptive_tau
```

### ğŸšï¸ **æ–¹æ¡ˆ3: æ”¹è¿›è´Ÿæ ·æœ¬ç”Ÿæˆç­–ç•¥** (ä¼˜å…ˆçº§ï¼šğŸŸ¡ ä¸­)

```python
def enhanced_negative_sampling(layer_actions, ground_truth_actions, noise_level=0.5):
    """å¢å¼ºçš„è´Ÿæ ·æœ¬ç”Ÿæˆ"""
    negatives = []
    
    # ç­–ç•¥1: å¤šå±‚åŠ¨ä½œé¢„æµ‹
    for i, layer_action in enumerate(layer_actions[:-1]):  # æ’é™¤æœ€åä¸€å±‚
        negatives.append(layer_action)
    
    # ç­–ç•¥2: é«˜æ–¯å™ªå£°æ‰°åŠ¨ï¼ˆå¤šä¸ªå™ªå£°æ°´å¹³ï¼‰
    for sigma in [0.1, 0.3, 0.5]:
        noise_actions = add_gaussian_noise(ground_truth_actions, sigma=sigma)
        negatives.append(noise_actions)
    
    # ç­–ç•¥3: éšæœºshuffleï¼ˆç ´åæ—¶åºï¼‰
    B, H, Da = ground_truth_actions.shape
    shuffle_idx = torch.randperm(H)
    shuffle_actions = ground_truth_actions[:, shuffle_idx, :]
    negatives.append(shuffle_actions)
    
    return torch.stack(negatives, dim=1)  # [B, M, H, Da]
```

### ğŸ”§ **æ–¹æ¡ˆ4: æ•°å€¼ç¨³å®šæ€§æ”¹è¿›** (ä¼˜å…ˆçº§ï¼šğŸŸ¡ ä¸­)

```python
def stable_energy_inbatch_swap_infonce(
    energy_model, h, a_pos, pad_mask, tau=2.0, eps=1e-8
):
    B, S, D = h.shape
    H, Da = a_pos.shape[1], a_pos.shape[2]
    
    # é¿å…å¤§å¼ é‡é‡å¤ï¼Œåˆ†æ‰¹è®¡ç®—
    E_ij = torch.zeros(B, B, device=h.device, dtype=h.dtype)
    
    for i in range(B):
        h_i = h[i:i+1].expand(B, -1, -1)  # [B, S, D]
        a_all = a_pos  # [B, H, Da]
        
        # ä¸ºæ¯ä¸ªh_iè®¡ç®—ä¸æ‰€æœ‰açš„èƒ½é‡
        E_i = energy_model(h_i, a_all, 
                          pad_mask[i:i+1].expand(B, -1) if pad_mask is not None else None)
        E_ij[i] = E_i.squeeze(-1)
    
    # æ•°å€¼ç¨³å®šçš„InfoNCE
    E_ij = E_ij + eps  # é¿å…é›¶å€¼
    logits = (-E_ij) / tau
    
    # é˜²æ­¢æ•°å€¼æº¢å‡º
    logits = torch.clamp(logits, min=-50, max=50)
    
    labels = torch.arange(B, device=h.device)
    loss = F.cross_entropy(logits, labels)
    
    return loss, torch.diag(E_ij).mean(), E_ij[~torch.eye(B, dtype=bool, device=h.device)].mean()
```

### âš–ï¸ **æ–¹æ¡ˆ5: æ··åˆæŸå¤±ç­–ç•¥** (ä¼˜å…ˆçº§ï¼šğŸŸ¢ ä½)

```python
def combined_energy_loss(energy_model, context_hidden, ground_truth_actions, 
                        layer_actions, energy_mask, step):
    """ç»“åˆå¤šç§æŸå¤±çš„ç­–ç•¥"""
    
    # ä¸»è¦æŸå¤±ï¼šæ”¹è¿›çš„in-batch InfoNCE
    inbatch_loss, E_pos, E_neg = stable_energy_inbatch_swap_infonce(
        energy_model, context_hidden, ground_truth_actions, energy_mask, tau=3.0
    )
    
    # è¾…åŠ©æŸå¤±1: æ­£è´Ÿæ ·æœ¬margin loss
    if len(layer_actions) > 1:
        E_pos = energy_model(context_hidden, ground_truth_actions, energy_mask)
        E_neg = energy_model(context_hidden, layer_actions[1], energy_mask)
        margin_loss = F.relu(E_neg - E_pos + 0.5).mean()  # margin = 0.5
    else:
        margin_loss = 0.0
    
    # è¾…åŠ©æŸå¤±2: èƒ½é‡èŒƒå›´æ­£åˆ™åŒ–ï¼ˆé˜²æ­¢collapseï¼‰
    energy_std = E_pos.std() + E_neg.std() if isinstance(E_neg, torch.Tensor) else E_pos.std()
    diversity_loss = F.relu(0.1 - energy_std)  # é¼“åŠ±èƒ½é‡å€¼å¤šæ ·åŒ–
    
    # åŠ¨æ€æƒé‡ï¼ˆæ—©æœŸæ›´ä¾èµ–marginï¼ŒåæœŸæ›´ä¾èµ–InfoNCEï¼‰
    alpha = min(1.0, step / 10000.0)  # ä»0é€æ¸å¢åŠ åˆ°1
    total_loss = alpha * inbatch_loss + (1-alpha) * margin_loss + 0.01 * diversity_loss
    
    return total_loss, E_pos.mean(), E_neg.mean() if isinstance(E_neg, torch.Tensor) else torch.tensor(0.0)
```

### ğŸ”„ **æ–¹æ¡ˆ6: è®­ç»ƒç­–ç•¥ä¼˜åŒ–** (ä¼˜å…ˆçº§ï¼šğŸŸ  é«˜)

```python
# åœ¨ finetune_Energy.py ä¸­çš„ä¿®æ”¹
def improved_training_strategy(cfg):
    """æ”¹è¿›çš„è®­ç»ƒç­–ç•¥"""
    
    # 1. æ›´ä¿å®ˆçš„å­¦ä¹ ç‡
    energy_optimizer = AdamW(
        energy_trainable_params, 
        lr=cfg.energy_learning_rate * 0.1,  # é™ä½10å€
        weight_decay=1e-4  # æ·»åŠ æƒé‡è¡°å‡
    )
    
    # 2. æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
    def energy_backward_step():
        if batch_idx >= cfg.energy_warm_steps:
            # å…ˆæ£€æŸ¥æ¢¯åº¦èŒƒæ•°
            grad_norm = torch.nn.utils.clip_grad_norm_(
                energy_model.parameters(), max_norm=0.5  # ä»1.0é™åˆ°0.5
            )
            
            # å¦‚æœæ¢¯åº¦èŒƒæ•°å¼‚å¸¸ï¼Œè·³è¿‡è¿™æ­¥
            if grad_norm > 10.0 or not torch.isfinite(grad_norm):
                energy_optimizer.zero_grad()
                return
                
            energy_optimizer.step()
            energy_optimizer.zero_grad()
    
    # 3. åŠ¨æ€warm-up
    dynamic_warm_steps = max(cfg.energy_warm_steps, 5000)  # è‡³å°‘5kæ­¥
    
    return energy_optimizer, energy_backward_step
```

## ğŸš€ **æ¨èå®æ–½é¡ºåº**

### é˜¶æ®µ1: ç´§æ€¥ä¿®å¤ (ç«‹å³æ‰§è¡Œ)
1. **ä¿®æ”¹èƒ½é‡è¾“å‡ºèŒƒå›´** - å°†Sigmoidæ”¹ä¸ºReLU + ç¼©æ”¾
2. **è°ƒæ•´Temperature** - tauä»0.5å¢åŠ åˆ°2.0-3.0
3. **åŠ å¼ºæ¢¯åº¦è£å‰ª** - max_normä»1.0é™åˆ°0.5

### é˜¶æ®µ2: ç¨³å®šæ€§å¢å¼º (1-2å¤©å†…)
1. **å®æ–½æ•°å€¼ç¨³å®šç‰ˆInfoNCE**
2. **é™ä½energyæ¨¡å‹å­¦ä¹ ç‡10å€**
3. **å¢åŠ energy warm-upæ­¥æ•°åˆ°10k**

### é˜¶æ®µ3: é«˜çº§ä¼˜åŒ– (1å‘¨å†…)
1. **æ”¹è¿›è´Ÿæ ·æœ¬ç”Ÿæˆç­–ç•¥**
2. **å®æ–½æ··åˆæŸå¤±ç­–ç•¥**
3. **æ·»åŠ èƒ½é‡å¤šæ ·æ€§æ­£åˆ™åŒ–**

## ğŸ” **è°ƒè¯•å»ºè®®**

### ç›‘æ§å…³é”®æŒ‡æ ‡:
```python
# æ·»åŠ åˆ°è®­ç»ƒå¾ªç¯ä¸­çš„ç›‘æ§ä»£ç 
if step % 100 == 0:
    with torch.no_grad():
        # ç›‘æ§èƒ½é‡å€¼åˆ†å¸ƒ
        E_sample = energy_model(context_hidden[:4], ground_truth_actions[:4], energy_mask[:4])
        print(f"Energy range: [{E_sample.min():.4f}, {E_sample.max():.4f}], std: {E_sample.std():.4f}")
        
        # ç›‘æ§logitsåˆ†å¸ƒ
        E_ij = compute_energy_matrix_sample(...)  # ç®€åŒ–ç‰ˆæœ¬
        logits = (-E_ij) / tau
        print(f"Logits range: [{logits.min():.4f}, {logits.max():.4f}]")
        
        # ç›‘æ§æ¢¯åº¦èŒƒæ•°
        energy_grad_norm = sum(p.grad.norm().item() for p in energy_model.parameters() if p.grad is not None)
        print(f"Energy grad norm: {energy_grad_norm:.6f}")
```

## ğŸ’¡ **æœ€ç»ˆå»ºè®®**

**ç«‹å³ä¿®æ”¹çš„å…³é”®ä»£ç **:
```python
# 1. åœ¨ EnergyModel.__init__ ä¸­
self.act = nn.Identity()  # æ›¿æ¢ nn.Sigmoid()
self.energy_scale = 5.0

# 2. åœ¨ EnergyModel.forward ä¸­  
E = F.softplus(raw * 0.1) * self.energy_scale + 1e-3  # æ›¿æ¢åŸæ¥çš„sigmoid

# 3. åœ¨ energy_inbatch_swap_infonce ä¸­
tau = 3.0  # æ›¿æ¢ 0.5

# 4. åœ¨è®­ç»ƒä»£ç ä¸­
energy_optimizer = AdamW(energy_trainable_params, lr=cfg.energy_learning_rate * 0.1)
torch.nn.utils.clip_grad_norm_(energy_model.parameters(), max_norm=0.5)
```

è¿™äº›ä¿®æ”¹åº”è¯¥èƒ½æ˜¾è‘—æ”¹å–„è®­ç»ƒç¨³å®šæ€§ï¼Œé¿å…NaNé—®é¢˜ï¼Œå¹¶æ¢å¤æ­£å¸¸çš„å­¦ä¹ æ›²çº¿ã€‚
