# åˆ†å¸ƒå¼Energyè®­ç»ƒé—®é¢˜è§£å†³æ–¹æ¡ˆ

## ğŸš¨ **é—®é¢˜è¯Šæ–­**

**ç—‡çŠ¶**: 8å¡ bs_local=3 æ—¶æ¨¡å‹å‡ ä¹ä¸å­¦ä¹ ï¼Œå•å¡ bs=24 æ—¶å­¦ä¹ è‰¯å¥½

**æ ¹æœ¬åŸå› **: `energy_inbatch_swap_infonce` æŸå¤±å‡½æ•°ä¸¥é‡ä¾èµ–äºæ‰¹å†…æ ·æœ¬çš„å¤šæ ·æ€§
- å•å¡bs=24: æ¯ä¸ªæ­£æ ·æœ¬æœ‰23ä¸ªè´Ÿæ ·æœ¬ï¼Œä¿¡å·å¼º
- 8å¡bs_local=3: æ¯ä¸ªGPUä¸Šæ¯ä¸ªæ­£æ ·æœ¬åªæœ‰2ä¸ªè´Ÿæ ·æœ¬ï¼Œä¿¡å·æå¼±

## ğŸ› ï¸ **è§£å†³æ–¹æ¡ˆ**

### æ–¹æ¡ˆ1: Global In-Batch InfoNCE (æ¨èâ­)

**åŸç†**: ä½¿ç”¨`all_gather`æ”¶é›†æ‰€æœ‰GPUçš„æ ·æœ¬ï¼Œè¿›è¡Œå…¨å±€InfoNCEè®¡ç®—

```python
def distributed_energy_inbatch_swap_infonce(
    energy_model, h, a_pos, pad_mask, tau=0.5, world_size=None
):
    """
    åˆ†å¸ƒå¼å‹å¥½çš„In-batch InfoNCEæŸå¤±
    """
    import torch.distributed as dist
    
    if world_size is None or world_size == 1:
        # å•å¡æƒ…å†µï¼Œç›´æ¥è°ƒç”¨åŸå‡½æ•°
        return energy_inbatch_swap_infonce(energy_model, h, a_pos, pad_mask, tau)
    
    # Step 1: æ”¶é›†æ‰€æœ‰GPUçš„æ ·æœ¬
    h_list = [torch.zeros_like(h) for _ in range(world_size)]
    a_list = [torch.zeros_like(a_pos) for _ in range(world_size)]
    pm_list = [torch.zeros_like(pad_mask) for _ in range(world_size)] if pad_mask is not None else None
    
    dist.all_gather(h_list, h)
    dist.all_gather(a_list, a_pos)
    if pad_mask is not None:
        dist.all_gather(pm_list, pad_mask)
    
    # Step 2: æ‹¼æ¥æˆå…¨å±€batch
    h_global = torch.cat(h_list, dim=0)  # [B*world_size, S, D]
    a_global = torch.cat(a_list, dim=0)  # [B*world_size, H, Da]
    pm_global = torch.cat(pm_list, dim=0) if pad_mask is not None else None
    
    # Step 3: è®¡ç®—å…¨å±€InfoNCE
    B_local = h.size(0)
    global_loss, E_pos_global, E_neg_global = energy_inbatch_swap_infonce(
        energy_model, h_global, a_global, pm_global, tau
    )
    
    # Step 4: åªå–å½“å‰GPUå¯¹åº”çš„æ­£æ ·æœ¬èƒ½é‡
    rank = dist.get_rank()
    start_idx = rank * B_local
    end_idx = start_idx + B_local
    
    E_pos_local = torch.diag(compute_local_energy_matrix(energy_model, h, a_pos, pad_mask))
    E_pos_mean = E_pos_local.mean()
    
    return global_loss, E_pos_mean, E_neg_global

def compute_local_energy_matrix(energy_model, h, a_pos, pad_mask):
    """è®¡ç®—æœ¬åœ°èƒ½é‡ç”¨äºç›‘æ§"""
    B, S, D = h.shape
    H, Da = a_pos.shape[1], a_pos.shape[2]
    
    h_rep = h.unsqueeze(1).expand(B, B, S, D).reshape(B*B, S, D)
    a_rep = a_pos.unsqueeze(0).expand(B, B, H, Da).reshape(B*B, H, Da)
    pm = pad_mask.unsqueeze(1).expand(B, B, -1).reshape(B*B, -1) if pad_mask is not None else None
    
    E_ij = energy_model(h_rep, a_rep, pm).view(B, B)
    return E_ij
```

**ä½¿ç”¨æ–¹æ³•**:
```python
# åœ¨ finetune_Energy.py ä¸­æ›¿æ¢ç¬¬536è¡Œ
# åŸæ¥ï¼š
# swap_loss, E_pos_mean, E_neg_mean = energy_inbatch_swap_infonce(
#     energy_model, context_hidden, ground_truth_actions, energy_mask
# )

# æ–°ç‰ˆæœ¬ï¼š
world_size = dist.get_world_size() if dist.is_initialized() else 1
swap_loss, E_pos_mean, E_neg_mean = distributed_energy_inbatch_swap_infonce(
    energy_model, context_hidden, ground_truth_actions, energy_mask, 
    tau=0.3, world_size=world_size
)
```

### æ–¹æ¡ˆ2: åŸºäºæ˜¾å¼è´Ÿæ ·æœ¬çš„InfoNCE

**åŸç†**: ä¸ä¾èµ–batchå†…æ ·æœ¬ï¼Œä½¿ç”¨layer actionsç”Ÿæˆå›ºå®šæ•°é‡çš„é«˜è´¨é‡è´Ÿæ ·æœ¬

```python
def layer_based_energy_infonce_loss(
    energy_model, context_hidden, ground_truth_actions, layer_actions, 
    energy_mask, tau=0.3, num_negatives=8
):
    """
    åŸºäºå±‚é—´é¢„æµ‹çš„InfoNCEæŸå¤±ï¼Œä¸ä¾èµ–batch size
    """
    B, H, Da = ground_truth_actions.shape
    
    # Step 1: ç”Ÿæˆå¤šæ ·åŒ–è´Ÿæ ·æœ¬
    negatives = []
    
    # ä»ä¸åŒå±‚é€‰æ‹©åŠ¨ä½œé¢„æµ‹ä½œä¸ºè´Ÿæ ·æœ¬
    if len(layer_actions) >= 2:
        # é€‰æ‹©å‰å‡ å±‚å’Œä¸­é—´å±‚çš„é¢„æµ‹
        selected_layers = [0, len(layer_actions)//3, len(layer_actions)//2, -2]
        for layer_idx in selected_layers:
            if layer_idx < len(layer_actions):
                negatives.append(layer_actions[layer_idx])
    
    # æ·»åŠ å™ªå£°æ‰°åŠ¨
    for sigma in [0.1, 0.2, 0.4]:
        noise_actions = ground_truth_actions + torch.randn_like(ground_truth_actions) * sigma
        negatives.append(noise_actions)
    
    # æ—¶åºshuffle
    shuffle_idx = torch.randperm(H)
    shuffle_actions = ground_truth_actions[:, shuffle_idx, :]
    negatives.append(shuffle_actions)
    
    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„è´Ÿæ ·æœ¬
    while len(negatives) < num_negatives:
        # æ·»åŠ æ›´å¤šéšæœºå™ªå£°
        sigma = 0.3 + 0.1 * len(negatives)
        noise_actions = ground_truth_actions + torch.randn_like(ground_truth_actions) * sigma
        negatives.append(noise_actions)
    
    # åªä¿ç•™æŒ‡å®šæ•°é‡çš„è´Ÿæ ·æœ¬
    negatives = negatives[:num_negatives]
    A_negatives = torch.stack(negatives, dim=1)  # [B, M, H, Da]
    
    # Step 2: è®¡ç®—InfoNCEæŸå¤±
    from energy.energy_model import energy_infonce_loss
    loss, E_pos_mean, E_neg_mean = energy_infonce_loss(
        energy_model, context_hidden, ground_truth_actions, A_negatives, 
        energy_mask, tau=tau
    )
    
    return loss, E_pos_mean, E_neg_mean
```

**ä½¿ç”¨æ–¹æ³•**:
```python
# åœ¨ run_forward_pass ä¸­æ›¿æ¢energyæŸå¤±è®¡ç®—
energy_loss, E_pos_mean, E_neg_mean = layer_based_energy_infonce_loss(
    energy_model, context_hidden, ground_truth_actions, layer_actions,
    energy_mask, tau=0.3, num_negatives=8
)
```

### æ–¹æ¡ˆ3: æ··åˆç­–ç•¥ (æœ€ç¨³å¥)

```python
def adaptive_energy_loss(
    energy_model, context_hidden, ground_truth_actions, layer_actions,
    energy_mask, local_batch_size, world_size=1, step=0
):
    """
    è‡ªé€‚åº”é€‰æ‹©æœ€ä½³çš„energyæŸå¤±ç­–ç•¥
    """
    effective_batch_size = local_batch_size * world_size
    
    # å†³ç­–é€»è¾‘
    if world_size == 1 or local_batch_size >= 8:
        # å•å¡æˆ–local batchè¶³å¤Ÿå¤§æ—¶ï¼Œä½¿ç”¨in-batchæ–¹æ³•
        if world_size > 1 and local_batch_size >= 6:
            # å¤šå¡ä½†local batchè¾ƒå¤§ï¼Œä½¿ç”¨åˆ†å¸ƒå¼in-batch
            loss, E_pos, E_neg = distributed_energy_inbatch_swap_infonce(
                energy_model, context_hidden, ground_truth_actions, energy_mask,
                tau=0.3, world_size=world_size
            )
        else:
            # å•å¡ï¼Œä½¿ç”¨æ ‡å‡†in-batch
            loss, E_pos, E_neg = energy_inbatch_swap_infonce(
                energy_model, context_hidden, ground_truth_actions, energy_mask, tau=0.3
            )
    else:
        # å¤šå¡ä¸”local batchå¾ˆå°ï¼Œä½¿ç”¨layer-basedæ–¹æ³•
        loss, E_pos, E_neg = layer_based_energy_infonce_loss(
            energy_model, context_hidden, ground_truth_actions, layer_actions,
            energy_mask, tau=0.3
        )
    
    return loss, E_pos, E_neg
```

### æ–¹æ¡ˆ4: å¢å¤§Local Batch Size

**æœ€ç®€å•ä½†å¯èƒ½å—å†…å­˜é™åˆ¶**ï¼š

```python
# è°ƒæ•´è®­ç»ƒé…ç½®
# 8å¡ bs_local=3 â†’ 4å¡ bs_local=6 æˆ– 2å¡ bs_local=12
# æˆ–è€…ä½¿ç”¨gradient accumulationå¢å¤§æœ‰æ•ˆbatch size

# åœ¨é…ç½®ä¸­ï¼š
batch_size: int = 6              # å¢åŠ åˆ°6
grad_accumulation_steps: int = 2 # æœ‰æ•ˆbatch size = 6*2 = 12 per GPU
```

## ğŸš€ **æ¨èå®æ–½ç­–ç•¥**

### ç«‹å³æµ‹è¯•æ–¹æ¡ˆ (ä¼˜å…ˆçº§æ’åº):

1. **æ–¹æ¡ˆ1 (Global InfoNCE)** - å¦‚æœé€šä¿¡å¼€é”€å¯æ¥å—
2. **æ–¹æ¡ˆ2 (Layer-based InfoNCE)** - å¦‚æœæ–¹æ¡ˆ1é€šä¿¡è¿‡äºæ˜‚è´µ  
3. **æ–¹æ¡ˆ4 (å¢å¤§local batch)** - å¦‚æœGPUå†…å­˜å……è¶³
4. **æ–¹æ¡ˆ3 (æ··åˆç­–ç•¥)** - ä½œä¸ºæœ€ç»ˆçš„é²æ£’è§£å†³æ–¹æ¡ˆ

### å®æ–½æ­¥éª¤:

1. **å…ˆè¯•æ–¹æ¡ˆ2** (æœ€ç®€å•ï¼Œé£é™©æœ€ä½):
   - ä¸éœ€è¦ä¿®æ”¹åˆ†å¸ƒå¼é€»è¾‘
   - ç›´æ¥æ›¿æ¢æŸå¤±å‡½æ•°å³å¯
   
2. **æ•ˆæœä¸ä½³å†è¯•æ–¹æ¡ˆ1** (æ•ˆæœæœ€å¥½ï¼Œä½†éœ€è¦é€šä¿¡):
   - éœ€è¦æ·»åŠ all_gatheré€šä¿¡
   - å¯èƒ½å¢åŠ è®­ç»ƒæ—¶é—´15-25%

3. **æœ€åè€ƒè™‘æ–¹æ¡ˆ4** (å¦‚æœå‰ä¸¤è€…éƒ½ä¸ç†æƒ³):
   - è°ƒæ•´ç¡¬ä»¶é…ç½®å’Œbatch size

## ğŸ“Š **é¢„æœŸæ•ˆæœ**

- **æ–¹æ¡ˆ1**: å®Œå…¨æ¢å¤å•å¡bs=24çš„å­¦ä¹ æ•ˆæœ
- **æ–¹æ¡ˆ2**: æä¾›ç¨³å®šçš„å­¦ä¹ ä¿¡å·ï¼Œä¸ä¾èµ–batch size
- **æ–¹æ¡ˆ4**: ç›´æ¥è§£å†³é—®é¢˜ï¼Œä½†å¯èƒ½å—å†…å­˜é™åˆ¶

é€‰æ‹©å“ªä¸ªæ–¹æ¡ˆä¸»è¦å–å†³äºæ‚¨çš„è®¡ç®—èµ„æºå’Œé€šä¿¡å¸¦å®½é™åˆ¶ã€‚
