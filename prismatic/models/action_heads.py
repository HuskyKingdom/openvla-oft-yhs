"""Implementations of various action heads, which serve as alternatives to VLM sequential token prediction."""

import math

import numpy as np
import torch
import torch.nn as nn
from diffusers.schedulers.scheduling_ddim import DDIMScheduler
from prismatic.vla.constants import ACTION_DIM, ACTION_TOKEN_BEGIN_IDX, IGNORE_INDEX, NUM_ACTIONS_CHUNK, PROPRIO_DIM, STOP_INDEX


class SinusoidalPositionalEncoding(nn.Module):
    """
    Sine- and cosine-based positional encoding that produces embeddings of a batch of timesteps.

    For example, at train time, the input might be a batch of 32 randomly sampled diffusion timesteps -> shape (32,)
    Then the output would be a batch of 32 timestep embeddings -> shape (32, D)

    Adapted from: https://github.com/real-stanford/diffusion_policy/blob/main/diffusion_policy/model/diffusion/positional_embedding.py
    """

    def __init__(self, dim):
        super().__init__()
        self.dim = dim  # dimensionality of the positional encoding

    def forward(self, x):
        # x: (batch_size,)
        device = x.device
        assert self.dim % 2 == 0, f"# dimensions must be even but got {self.dim}"
        half_dim = self.dim // 2
        exponent = torch.arange(half_dim, device=device) * -math.log(10000) / (half_dim - 1)  # shape: (D/2,)
        emb = torch.exp(exponent)  # shape: (D/2,)
        emb = x[:, None] * emb[None, :]  # shape: (batch_size, 1) * (1, D/2) -> (batch_size, D/2)
        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)  # shape: (batch_size, D)
        return emb


class MLPResNetBlock(nn.Module):
    """One MLP ResNet block with a residual connection."""
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
        self.ffn = nn.Sequential(  # feedforward network, similar to the ones in Transformers
            nn.LayerNorm(dim),
            nn.Linear(dim, dim),
            nn.ReLU(),
        )

    def forward(self, x):
        # x: (batch_size, hidden_dim)
        # We follow the module ordering of "Pre-Layer Normalization" feedforward networks in Transformers as
        # described here: https://arxiv.org/pdf/2002.04745.pdf
        identity = x
        x = self.ffn(x)
        x = x + identity
        return x


class MLPResNet(nn.Module):
    """MLP with residual connection blocks."""
    def __init__(self, num_blocks, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.layer_norm1 = nn.LayerNorm(input_dim)
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.mlp_resnet_blocks = nn.ModuleList()
        for _ in range(num_blocks):
            self.mlp_resnet_blocks.append(MLPResNetBlock(dim=hidden_dim))
        self.layer_norm2 = nn.LayerNorm(hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # x: (batch_size, input_dim)
        x = self.layer_norm1(x)  # shape: (batch_size, input_dim)
        x = self.fc1(x)  # shape: (batch_size, hidden_dim)
        x = self.relu(x)  # shape: (batch_size, hidden_dim)
        for block in self.mlp_resnet_blocks:
            x = block(x)  # shape: (batch_size, hidden_dim)
        x = self.layer_norm2(x)  # shape: (batch_size, hidden_dim)
        x = self.fc2(x)  # shape: (batch_size, output_dim)
        return x


class L1RegressionActionHead(nn.Module):
    """
    Simple MLP-based action head that generates continuous actions via L1 regression.
    
    Note: ACTION_DIM now includes EOS flag as the last dimension.
    Input: hidden states from BASE_ACTION_DIM tokens (7, excluding EOS token)
    Output: predictions for ACTION_DIM values (8, including EOS flag)
    
    This is because during training, action tokens (7 dims) are selected by mask,
    but EOS token is not included in the mask (EOS token ID < ACTION_TOKEN_BEGIN_IDX).
    """
    def __init__(
        self,
        input_dim=4096,
        hidden_dim=4096,
        action_dim=None,  # Will use ACTION_DIM from constants if not specified
    ):
        super().__init__()
        # Use ACTION_DIM from constants if action_dim not specified
        if action_dim is None:
            action_dim = ACTION_DIM
        self.action_dim = action_dim
        
        # CRITICAL: input_dim is based on BASE_ACTION_DIM (7), not ACTION_DIM (8)
        # because EOS token is not included in action_mask during training
        from prismatic.vla.constants import BASE_ACTION_DIM
        self.model = MLPResNet(
            num_blocks=2, 
            input_dim=input_dim*BASE_ACTION_DIM,  # Use BASE_ACTION_DIM (7) for input
            hidden_dim=hidden_dim, 
            output_dim=action_dim  # Output ACTION_DIM (8) including EOS
        )

    def predict_action(self, actions_hidden_states):
        # actions_hidden_states: last hidden states of Transformer corresponding to action tokens in sequence
        # - shape: (batch_size, NUM_ACTIONS_CHUNK * BASE_ACTION_DIM, hidden_dim)
        #   Note: Only BASE_ACTION_DIM (7) tokens per action, EOS not included in mask
        # Output:
        # - shape: (batch_size, NUM_ACTIONS_CHUNK, ACTION_DIM)
        #   Note: Predicts ACTION_DIM (8) values including EOS flag
        batch_size = actions_hidden_states.shape[0]
        device = actions_hidden_states.device
        # Reshape: (B, NUM_ACTIONS_CHUNK * BASE_ACTION_DIM, hidden_dim) 
        #       -> (B, NUM_ACTIONS_CHUNK, BASE_ACTION_DIM * hidden_dim)
        rearranged_actions_hidden_states = actions_hidden_states.reshape(batch_size, NUM_ACTIONS_CHUNK, -1)
        # Model predicts ACTION_DIM (8) values from BASE_ACTION_DIM (7) tokens
        action = self.model(rearranged_actions_hidden_states)
        
        # [CRITICAL] Apply sigmoid to EOS flag (8th dimension) to constrain to [0, 1]
        # This prevents numerical instability from unbounded EOS predictions
        from prismatic.vla.constants import BASE_ACTION_DIM
        if self.action_dim > BASE_ACTION_DIM:
            # Split: first 7 dims (base actions) + 8th dim (EOS flag)
            base_actions = action[..., :BASE_ACTION_DIM]
            eos_flag = torch.sigmoid(action[..., BASE_ACTION_DIM:])  # Constrain to [0, 1]
            action = torch.cat([base_actions, eos_flag], dim=-1)
        
        return action


class NoisePredictionModel(nn.Module):
    """
    Diffusion noise prediction model that takes an observation embedding (which fuses the
    noisy action, diffusion timestep, and image-language observation embeddings) and
    outputs a noise prediction.
    """

    def __init__(
        self,
        transformer_hidden_dim,  # Transformer hidden embedding size
        hidden_dim,  # MLP hidden size
        action_dim=7,  # action dimensionality
    ):
        super().__init__()
        self.mlp_resnet = MLPResNet(
            num_blocks=2,
            input_dim=transformer_hidden_dim,
            hidden_dim=hidden_dim,
            output_dim=action_dim,
        )

    def forward(
        self,
        obs,
    ):
        # obs: observation embeddings to condition the generation on
        # - shape: (batch_size, chunk_len, rearranged_hidden_dim=action_dim*hidden_dim)
        #
        # output: predicted noise
        # - shape: (batch_size, action_dim)
        output = self.mlp_resnet(obs)
        return output


class DiffusionActionHead(nn.Module):
    """
    Simple MLP-based action head that generates continuous actions via conditional denoising diffusion process.

    Loosely inspired by: https://github.com/real-stanford/diffusion_policy/blob/main/diffusion_policy/model/diffusion/transformer_for_diffusion.py
    """

    def __init__(
        self,
        input_dim=4096,
        hidden_dim=4096,
        action_dim=None,  # Will use ACTION_DIM from constants if not specified
        num_diffusion_steps_train=50,
    ):
        super().__init__()
        # Use ACTION_DIM from constants if action_dim not specified
        if action_dim is None:
            from prismatic.vla.constants import ACTION_DIM
            action_dim = ACTION_DIM
        self.action_dim = action_dim
        
        # CRITICAL: Use BASE_ACTION_DIM (7) for transformer hidden dim
        from prismatic.vla.constants import BASE_ACTION_DIM
        self.noise_predictor = NoisePredictionModel(
            transformer_hidden_dim=hidden_dim*BASE_ACTION_DIM,  # Use BASE_ACTION_DIM
            hidden_dim=hidden_dim, 
            action_dim=action_dim  # Output ACTION_DIM including EOS
        )
        self.num_diffusion_steps_train = num_diffusion_steps_train
        self.noise_scheduler = DDIMScheduler(num_train_timesteps=num_diffusion_steps_train, beta_schedule="squaredcos_cap_v2")
        self.time_encoder = SinusoidalPositionalEncoding(dim=hidden_dim)

    def sample_noisy_actions(self, ground_truth_actions):
        """
        Samples noise and applies noise to ground-truth actions to produce noisy actions, which are
        used as input in the noise prediction network. Returns noise, noisy actions, and the
        corresponding diffusion timestep embeddings.
        """
        # ground_truth_actions: ground-truth actions
        # - shape: (batch_size, chunk_len, action_dim)
        batch_size = ground_truth_actions.shape[0]
        device = ground_truth_actions.device
        # Sample random noise with shape equal to actions, used for closed-form forward diffusion.
        noise = torch.randn(size=(batch_size, NUM_ACTIONS_CHUNK, ACTION_DIM), device=device, dtype=ground_truth_actions.dtype)  # (B, chunk_len, action_dim)
        # Sample random diffusion timesteps (one for each action in batch).
        timesteps = torch.randint(
            low=0, high=self.noise_scheduler.config.num_train_timesteps, size=(batch_size,), device=device
        )
        # Add noise to clean actions according to the magnitude at each diffusion timestep via
        # closed-form forward diffusion.
        noisy_actions = self.noise_scheduler.add_noise(ground_truth_actions, noise, timesteps)  # (B, chunk_len, action_dim)

        # Get diffusion timestep embeddings as well
        diffusion_timestep_embeddings = self.time_encoder(timesteps).to(noisy_actions.dtype).to(noisy_actions.device)  # (B, llm_dim)
        diffusion_timestep_embeddings = diffusion_timestep_embeddings.unsqueeze(1)  # (B, 1, llm_dim)

        return_dict = dict(
            noise=noise,
            noisy_actions=noisy_actions,
            diffusion_timestep_embeddings=diffusion_timestep_embeddings,
        )

        return return_dict

    def predict_noise(self, actions_hidden_states):
        """
        Given a batch of last hidden Transformer layer embeddings (which fuse the vision-language observation embeddings,
        noisy action embeddings, and diffusion timestep embedding), predicts the noise applied to the actions.
        """
        # actions_hidden_states: last hidden states of Transformer corresponding to action tokens in sequence
        # - shape: (batch_size, chunk_len * action_dim, hidden_dim)
        batch_size = actions_hidden_states.shape[0]
        device = actions_hidden_states.device
        rearranged_actions_hidden_states = actions_hidden_states.reshape(batch_size, NUM_ACTIONS_CHUNK, -1)  # (batch_size, chunk_len, action_dim * hidden_dim)
        # Get diffusion model's noise prediction.
        noise_pred = self.noise_predictor(rearranged_actions_hidden_states)
        return noise_pred
