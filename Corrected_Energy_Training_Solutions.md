# Energyè®­ç»ƒé—®é¢˜çš„ä¿®æ­£è§£å†³æ–¹æ¡ˆ

## é—®é¢˜é‡æ–°å®šä½

**çœŸæ­£çš„æ ¹æœ¬é—®é¢˜**: ä¸æ˜¯Ï„å¤ªå°æˆ–èƒ½é‡èŒƒå›´å¤ªå¤§ï¼Œè€Œæ˜¯ï¼š
1. **èƒ½é‡åŒºåˆ†åº¦ä¸è¶³** - æ­£è´Ÿæ ·æœ¬èƒ½é‡å·®å¼‚å¤ªå°
2. **Sigmoidé¥±å’Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±**
3. **è´Ÿæ ·æœ¬è´¨é‡å·®**

## ğŸ¯ **ä¿®æ­£è§£å†³æ–¹æ¡ˆ**

### æ–¹æ¡ˆ1: ä¿å®ˆçš„èƒ½é‡èŒƒå›´è°ƒæ•´ (ğŸ”´æ¨è)

```python
class EnergyModel(nn.Module):
    def __init__(self, ...):
        # ä¸è¦å®Œå…¨ç§»é™¤Sigmoidï¼Œè€Œæ˜¯æ‰©å¤§å…¶è¾“å…¥èŒƒå›´
        self.act = nn.Sigmoid()
        self.energy_scale = 2.0  # æ¸©å’Œæ‰©å¤§èŒƒå›´åˆ° (0, 2)
        self.energy_offset = 0.1  # é¿å…è¿‡å°å€¼
        
    def forward(self, hN, a, pad_mask=None):
        # ... å‰é¢è®¡ç®—ç›¸åŒ ...
        raw = self.prediction_head(energy)
        
        # æ¸©å’Œæ‰©å¤§Sigmoidè¾“å…¥èŒƒå›´ï¼Œé¿å…é¥±å’Œ
        scaled_raw = raw * 0.5  # å°†rawç¼©æ”¾åˆ°åˆç†èŒƒå›´
        E = self.act(scaled_raw) * self.energy_scale + self.energy_offset
        # ç»“æœ: E âˆˆ [0.1, 2.1]ï¼Œæ¯”åŸæ¥çš„[1e-6, 1+1e-6]å¤§ï¼Œä½†ä¸ä¼šå¤ªæç«¯
        
        return E
```

**ä¼˜åŠ¿**:
- èƒ½é‡èŒƒå›´é€‚åº¦æ‰©å¤§: (0.1, 2.1)
- é¿å…Sigmoidå®Œå…¨é¥±å’Œ
- æ¢¯åº¦scaleå¢åŠ 2å€ï¼Œå¯æ§
- ä¿æŒæ•°å€¼ç¨³å®šæ€§

### æ–¹æ¡ˆ2: æ”¹è¿›è´Ÿæ ·æœ¬ç”Ÿæˆ (ğŸ”´æœ€é‡è¦)

```python
def improved_negative_sampling(layer_actions, ground_truth_actions, context_hidden, energy_model):
    """å…³é”®: ç”Ÿæˆæ›´æœ‰åŒºåˆ†åº¦çš„è´Ÿæ ·æœ¬"""
    
    # ç­–ç•¥1: é€‰æ‹©èƒ½é‡æ¥è¿‘ä½†ä¸ç›¸åŒçš„è´Ÿæ ·æœ¬
    with torch.no_grad():
        # è®¡ç®—æ‰€æœ‰å±‚åŠ¨ä½œçš„èƒ½é‡
        layer_energies = []
        for layer_action in layer_actions[:-1]:  # æ’é™¤æœ€ç»ˆå±‚
            energy = energy_model(context_hidden, layer_action)
            layer_energies.append((layer_action, energy))
        
        # æŒ‰èƒ½é‡æ’åºï¼Œé€‰æ‹©ä¸­ç­‰èƒ½é‡çš„ä½œä¸ºè´Ÿæ ·æœ¬ï¼ˆä¸æ˜¯æœ€å·®çš„ï¼‰
        layer_energies.sort(key=lambda x: x[1].mean().item())
        mid_idx = len(layer_energies) // 2
        selected_negative = layer_energies[mid_idx][0]
    
    # ç­–ç•¥2: æ·»åŠ æ§åˆ¶å¼ºåº¦çš„å™ªå£°
    B, H, Da = ground_truth_actions.shape
    noise_levels = [0.1, 0.3]  # ä¸åŒå¼ºåº¦çš„å™ªå£°
    noise_negatives = []
    for sigma in noise_levels:
        noise_action = ground_truth_actions + torch.randn_like(ground_truth_actions) * sigma
        noise_negatives.append(noise_action)
    
    # ç»„åˆè´Ÿæ ·æœ¬
    all_negatives = [selected_negative] + noise_negatives
    return torch.stack(all_negatives, dim=1)  # [B, M, H, Da]
```

### æ–¹æ¡ˆ3: åŠ¨æ€æ¸©åº¦è°ƒæ•´ (ğŸŸ¡å¯é€‰)

```python
def adaptive_energy_inbatch_swap_infonce(energy_model, h, a_pos, pad_mask, base_tau=0.5):
    """è‡ªé€‚åº”æ¸©åº¦è°ƒæ•´"""
    
    B, S, D = h.shape
    H, Da = a_pos.shape[1], a_pos.shape[2]
    
    # è®¡ç®—èƒ½é‡çŸ©é˜µ
    h_rep = h.unsqueeze(1).expand(B, B, S, D).reshape(B*B, S, D)
    a_rep = a_pos.unsqueeze(0).expand(B, B, H, Da).reshape(B*B, H, Da)
    pm = pad_mask.unsqueeze(1).expand(B, B, -1).reshape(B*B, -1) if pad_mask is not None else None
    
    E_ij = energy_model(h_rep, a_rep, pm).view(B, B).squeeze(-1)
    
    # åŠ¨æ€è°ƒæ•´æ¸©åº¦
    E_pos = torch.diag(E_ij)  # æ­£æ ·æœ¬èƒ½é‡
    E_neg_mask = ~torch.eye(B, dtype=bool, device=h.device)
    E_neg = E_ij[E_neg_mask]  # è´Ÿæ ·æœ¬èƒ½é‡
    
    # åŸºäºèƒ½é‡å·®å¼‚è°ƒæ•´æ¸©åº¦
    energy_diff = (E_neg.mean() - E_pos.mean()).abs()
    if energy_diff < 0.1:  # å·®å¼‚å¤ªå°
        adaptive_tau = base_tau * 0.5  # é™ä½æ¸©åº¦å¢å¼ºå¯¹æ¯”
    elif energy_diff > 1.0:  # å·®å¼‚å¾ˆå¤§
        adaptive_tau = base_tau * 2.0  # å‡é«˜æ¸©åº¦ç¼“è§£è¿‡åº¦å¯¹æ¯”
    else:
        adaptive_tau = base_tau
    
    # é™åˆ¶æ¸©åº¦èŒƒå›´
    adaptive_tau = torch.clamp(torch.tensor(adaptive_tau), 0.1, 2.0).item()
    
    logits = (-E_ij) / adaptive_tau
    labels = torch.arange(B, device=h.device)
    loss = F.cross_entropy(logits, labels)
    
    return loss, E_pos.mean(), E_neg.mean(), adaptive_tau
```

### æ–¹æ¡ˆ4: æ¸è¿›å¼è®­ç»ƒç­–ç•¥ (ğŸŸ é‡è¦)

```python
def progressive_energy_training(energy_model, step, total_steps):
    """æ¸è¿›å¼è®­ç»ƒé¿å…çªç„¶å˜åŒ–"""
    
    # é˜¶æ®µ1: å‰5kæ­¥ï¼Œé™ä½å­¦ä¹ ç‡ï¼Œä¸“æ³¨ç¨³å®šæ€§
    if step < 5000:
        lr_scale = 0.1
        grad_clip = 0.3
    # é˜¶æ®µ2: 5k-15kæ­¥ï¼Œé€æ­¥æé«˜å­¦ä¹ ç‡
    elif step < 15000:
        progress = (step - 5000) / 10000
        lr_scale = 0.1 + 0.4 * progress  # ä»0.1å¢é•¿åˆ°0.5
        grad_clip = 0.3 + 0.2 * progress  # ä»0.3å¢é•¿åˆ°0.5
    # é˜¶æ®µ3: 15k+æ­¥ï¼Œæ­£å¸¸å­¦ä¹ ç‡
    else:
        lr_scale = 0.5
        grad_clip = 0.5
    
    return lr_scale, grad_clip
```

## ğŸš€ **æœ€ç»ˆæ¨èçš„ä¿®æ”¹**

**ç«‹å³å®æ–½ (ä¼˜å…ˆçº§ğŸ”´)**:

```python
# 1. åœ¨ EnergyModel.__init__ ä¸­
self.energy_scale = 2.0  # è€Œä¸æ˜¯5.0
self.energy_offset = 0.1

# 2. åœ¨ EnergyModel.forward ä¸­  
scaled_raw = raw * 0.5  # é˜²æ­¢Sigmoidé¥±å’Œ
E = self.act(scaled_raw) * self.energy_scale + self.energy_offset

# 3. ä¿æŒåŸå§‹æ¸©åº¦æˆ–ç¨å¾®é™ä½
tau = 0.3  # ä»0.5é™åˆ°0.3ï¼Œå¢å¼ºå¯¹æ¯”ä¿¡å·

# 4. æ›´ä¿å®ˆçš„å­¦ä¹ ç‡å’Œæ¢¯åº¦è£å‰ª
energy_lr = cfg.energy_learning_rate * 0.1  # é™ä½å­¦ä¹ ç‡
torch.nn.utils.clip_grad_norm_(energy_model.parameters(), max_norm=0.3)

# 5. æ”¹è¿›è´Ÿæ ·æœ¬ç”Ÿæˆ
negatives = improved_negative_sampling(layer_actions, ground_truth_actions, 
                                     context_hidden, energy_model)
```

**ä¸ºä»€ä¹ˆè¿™æ ·ä¿®æ”¹æ›´åˆç†**:

1. **èƒ½é‡èŒƒå›´**: (0.1, 2.1) vs åŸæ¥çš„(1e-6, 1.000001)
   - åŒºåˆ†åº¦æé«˜**2000å€**ä½†æ¢¯åº¦åªå¢å¤§2å€ï¼Œå¹³è¡¡
   
2. **æ¸©åº¦ç³»æ•°**: 0.3 vs åŸæ¥çš„0.5
   - æ‚¨è¯´å¾—å¯¹ï¼Œé™ä½Ï„å¢å¼ºå¯¹æ¯”ä¿¡å·
   
3. **æ¸è¿›ç­–ç•¥**: é¿å…è®­ç»ƒéœ‡è¡
   - ä»ä¿å®ˆå¼€å§‹ï¼Œé€æ­¥æ”¾å¼€çº¦æŸ

4. **è´Ÿæ ·æœ¬è´¨é‡**: è¿™æ˜¯æœ€å…³é”®çš„
   - å¥½çš„è´Ÿæ ·æœ¬æ¯”å¤§çš„èƒ½é‡èŒƒå›´æ›´é‡è¦

## ğŸ“Š **é¢„æœŸæ•ˆæœ**

- **èƒ½é‡å·®å¼‚**: ä»0.1å¢åŠ åˆ°0.5-1.0
- **æ¢¯åº¦ç¨³å®š**: é¿å…çˆ†ç‚¸ï¼Œä¿æŒå¯å­¦ä¹ æ¢¯åº¦
- **å­¦ä¹ æ›²çº¿**: åº”è¯¥çœ‹åˆ°smoothä¸‹é™è€Œä¸æ˜¯ç›´çº¿
- **NaNé¿å…**: æ¸è¿›ç­–ç•¥å¤§å¹…é™ä½NaNé£é™©

è¿™ä¸ªä¿®æ­£æ–¹æ¡ˆè§£å†³äº†æ‚¨æ‹…å¿ƒçš„ä¸¤ä¸ªé—®é¢˜ï¼ŒåŒæ—¶ä¿æŒè®­ç»ƒç¨³å®šæ€§ï¼
